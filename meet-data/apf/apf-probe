#!/usr/bin/env python3
# vim: set ts=8 sts=4 et sw=4 tw=99:
#
# Probes for new APF/AAPF/WPC meets.
# Also checks the results archive.


from bs4 import BeautifulSoup
import datetime
import os
import shutil
import sys
import urllib.request


URL = "http://worldpowerliftingcongress.com/wpc-results/"
ARCHIVEURL = "http://worldpowerliftingcongress.com/results-archive/"

BASEURL = "http://worldpowerliftingcongress.com"
FEDDIR = os.path.dirname(os.path.realpath(__file__))


def error(msg):
    print(msg, file=sys.stderr)
    sys.exit(1)


def color(s):
    return "\033[1;37m" + s + "\033[0;m"


def gethtml(url):
    with urllib.request.urlopen(url) as r:
        return r.read()


def getmeetlist(html):
    soup = BeautifulSoup(html, 'html.parser')

    main = soup.find("div", {"id": "main-content"})
    divs = main.find_all("div", {"class": "wpb_wrapper"})
    if len(divs) == 0:
        error("Page layout seems to have changed.")

    urls = []

    for div in divs:
        for a in div.find_all('a'):
            url = a['href']
            if not 'http' in url:
                url = BASEURL + '/' + url

            # Remove some false positives.
            if '/#' in url:
                continue

            if not url in urls:
                urls.append(url.strip())

    return urls


def getenteredurls():
    urls = []
    for dirname, subdirs, files in os.walk(FEDDIR):
        if 'URL' in files:
            with open(dirname + os.sep + 'URL', 'r') as fd:
                for k in fd.readlines():
                    urls.append(k.strip())
    return urls


def main():
    html = gethtml(URL)
    meetlist = getmeetlist(html)

    html = gethtml(ARCHIVEURL)
    meetlist = meetlist + getmeetlist(html)

    known = getenteredurls()

    for m in meetlist:
        if not m in known:
            print(color('[APF] ') + m)


if __name__ == '__main__':
    main()
